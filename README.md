# Список моделей LLM для локального использования

Этот список включает модели LLM (Large Language Models) с открытым исходным кодом, доступные для локального развертывания. Модели охватывают различные языки, включая русский и китайский, и могут быть использованы для широкого спектра задач, таких как генерация текста, анализ, кодирование и другие. В таблице указаны их характеристики, размеры файлов, лицензии и ссылки для скачивания.

| **Model Name**             | **Description**                                                               | **Parameter Size** | **File Size**  | **License**   | **Source**     | **Download Link**                                                               |
|----------------------------|-------------------------------------------------------------------------------|--------------------|----------------|---------------|----------------|--------------------------------------------------------------------------------|
| **RuGPT-3.5 13B**          | Русскоязычная версия GPT-3.5 с 13 миллиардами параметров.                     | 13B                | ~7.45 GB       | MIT           | Reddit         | [Hugging Face](https://huggingface.co/ai-forever/ruGPT-3.5-13B)                |
| **Vikhr**                  | Модели, ориентированные на русский язык, с улучшенной производительностью.    | various            | ~6.97 GB       | OpenRAIL-M    | arXiv          | [Hugging Face](https://huggingface.co/vikhr)                                    |
| **T-Lite и T-Pro**         | Модели, адаптированные для русского языка с использованием технологии Continual Pretraining. | various            | ~7.61 GB       | OpenRAIL-M    | Известия       | [GitHub](https://huggingface.co/t-tech)                                  |
| **GigaChat**               | Модели с 100 миллиардами параметров, обученные на русском и английском языках. | 100B               | ~20.6 GB       | OpenRAIL-M    | arXiv          | [GitHub](https://github.com/yandex/gigachat)                                    |
| **YaLM-100B**              | Модели с открытым исходным кодом, от 7B до 65B параметров.             | 100B               | ~200 GB        | Apache 2.0    | Wikipedia      | [Hugging Face](https://huggingface.co/Yandex/YaLM-100B)                        |
| **LLaMA 3.2 7B**           | Модели с открытым исходным кодом, от 7B до 65B параметров.             | 7B                 | ~2.5 GB        | Meta Research | Wikipedia      | [Hugging Face](https://huggingface.co/meta-llama/Llama-3.2-1B)                 |
| **Mistral 7B**             | Модели с открытым исходным кодом, оптимизированные для быстродействия и эффективности. | 7.3B               | ~13.74 GB      | Apache 2.0    | Wikipedia      | [Hugging Face](https://huggingface.co/mistralai/Mistral-7B-v0.1)               |
| **BLOOMZ 7B**              | Модели для многоязычных задач, поддерживающая китайский и другие языки.       | 7.1B               | ~13.17 GB      | OpenRAIL-M    | BigScience     | [Hugging Face](https://huggingface.co/bigscience/bloomz-7b1-mt)                |
| **GPT-NeoX-20B**           | Модели с открытым исходным кодом для обработки большого объема текста.        | 20B                | ~40 GB         | Apache 2.0    | EleutherAI     | [Hugging Face](https://huggingface.co/EleutherAI/gpt-neox-20b)                  |
| **mistral-small-3.2**      | Модели с квантованием для быстродействия и меньших требований к памяти.        | 3.2B               | ~6.4 GB        | Apache 2.0    | Mistral        | [Hugging Face](https://huggingface.co/mistralai/mistral-small-3.2)             |
| **mistral-small**          | Модели с улучшениями, доступные в разных размерах и вариантах.      | 7.3B               | ~13.74 GB      | Apache 2.0    | Mistral        | [Hugging Face](https://huggingface.co/mistralai/mistral-small)                 |
| **devstral-small-2505**    | Модели для завершения кода и генерации с оптимизацией.                         | 2.5B               | ~5 GB          | Apache 2.0    | EleutherAI     | [Hugging Face](https://huggingface.co/EleutherAI/devstral-small-2505)           |
| **mistral-7b-instruct-v0.3**| Модели с инструкциями для повышения точности.                      | 7B                 | ~14 GB         | Apache 2.0    | Mistral        | [Hugging Face](https://huggingface.co/mistralai/mistral-7b-instruct-v0.3)       |
| **mistral-12b**            | Модели с улучшениями для обработки текста.                                     | 12B                | ~24 GB         | Apache 2.0    | Mistral        | [Hugging Face](https://huggingface.co/mistralai/mistral-12b)                   |
| **mistral-14b**            | Модели с расширенными параметрами для генерации текста.                        | 14B                | ~28 GB         | Apache 2.0    | Mistral        | [Hugging Face](https://huggingface.co/mistralai/mistral-14b)                   |
| **codestral-22b-v0.1**     | Модели с поддержкой 80 языков программирования.                                | 22B                | ~44 GB         | Apache 2.0    | Mistral        | [Hugging Face](https://huggingface.co/mistralai/codestral-22b-v0.1)            |
| **phi-4-mini-reasoning**   | Легковесная модель Phi для эффективного вычисления.                           | 4B                 | ~8 GB          | Apache 2.0    | Phi            | [Hugging Face](https://huggingface.co/phi-4-mini-reasoning)                     |
| **phi-4-reasoning-plus**   | Улучшенная версия Phi с дополнительным обучением для точности.                | 4B                 | ~8 GB          | Apache 2.0    | Phi            | [Hugging Face](https://huggingface.co/phi-4-reasoning-plus)                     |
| **phi-4**                 | Полная версия Phi для более глубоких вычислений.                               | 4B                 | ~8 GB          | Apache 2.0    | Phi            | [Hugging Face](https://huggingface.co/phi-4)                                    |
| **qwen3-235b-a22b**        | Модели с поддержкой больших объемов данных и высокоэффективных вычислений.    | 235B               | ~470 GB        | Apache 2.0    | Qwen           | [Hugging Face](https://huggingface.co/qwen/qwen3-235b-a22b)                     |
| **qwen3-32b**              | Модели для масштабных проектов с большой производительностью.                 | 32B                | ~64 GB         | Apache 2.0    | Qwen           | [Hugging Face](https://huggingface.co/qwen/qwen3-32b)                          |
| **qwen3-30b-a3b**          | Модели для работы с большими объемами текста и данными.                       | 30B                | ~60 GB         | Apache 2.0    | Qwen           | [Hugging Face](https://huggingface.co/qwen/qwen3-30b-a3b)                      |
| **qwen3-1.7b**             | Модели для базовых задач генерации текста.                                    | 1.7B               | ~3.4 GB        | Apache 2.0    | Qwen           | [Hugging Face](https://huggingface.co/qwen/qwen3-1.7b)                         |
| **qwen3-4b**               | Модели для решения сложных задач с поддержкой многозадачности.                | 4B                 | ~8 GB          | Apache 2.0    | Qwen           | [Hugging Face](https://huggingface.co/qwen/qwen3-4b)                           |
| **qwen3-14b**              | Модели с расширенной памятью для эффективной генерации текста.                | 14B                | ~28 GB         | Apache 2.0    | Qwen           | [Hugging Face](https://huggingface.co/qwen/qwen3-14b)                          |
| **qwen3-8b**               | Модели для решения среднего уровня задач.                                     | 8B                 | ~16 GB         | Apache 2.0    | Qwen           | [Hugging Face](https://huggingface.co/qwen/qwen3-8b)                           |
| **qwen2.5-vl-7b**          | Модели с квантованием для улучшения скорости обработки.                       | 7B                 | ~14 GB         | Apache 2.0    | Qwen           | [Hugging Face](https://huggingface.co/qwen/qwen2.5-vl-7b)                      |
| **qwen2.5-coder-14b**      | Модели для генерации кода и исправлений в больших проектах.                   | 14B                | ~28 GB         | Apache 2.0    | Qwen           | [Hugging Face](https://huggingface.co/qwen/qwen2.5-coder-14b)                  |
| **qwen2.5-coder-32b**      | Модели для работы с кодом и решениями в масштабных системах.                  | 32B                | ~64 GB         | Apache 2.0    | Qwen           | [Hugging Face](https://huggingface.co/qwen/qwen2.5-coder-32b)                  |
| **gemma-3-27b**           | Модели для высококачественной генерации текста в сложных задачах.             | 27B                | ~54 GB         | Apache 2.0    | Gemma          | [Hugging Face](https://huggingface.co/gemma/gemma-3-27b)                       |
| **gemma-3-12b**           | Модели для текстовых данных среднего объема.                                  | 12B                | ~24 GB         | Apache 2.0    | Gemma          | [Hugging Face](https://huggingface.co/gemma/gemma-3-12b)                       |
| **gemma-3-4b**            | Модели для небольших задач генерации и анализа.                               | 4B                 | ~8 GB          | Apache 2.0    | Gemma          | [Hugging Face](https://huggingface.co/gemma/gemma-3-4b)                        |
| **gemma-3-1b**            | Модели для быстрых и эффективных решений.                                     | 1B                 | ~2 GB          | Apache 2.0    | Gemma          | [Hugging Face](https://huggingface.co/gemma/gemma-3-1b)                        |
| **gemma-2-9b**            | Модели для базовых и промежуточных задач.                                     | 9B                 | ~18 GB         | Apache 2.0    | Gemma          | [Hugging Face](https://huggingface.co/gemma/gemma-2-9b)                        |
| **gemma-2-27b**           | Модели для сложных задач с большим объемом данных.                            | 27B                | ~54 GB         | Apache 2.0    | Gemma          | [Hugging Face](https://huggingface.co/gemma/gemma-2-27b)                       |
| **granite-3.2-8b**        | Модели для сложных вычислений и текстовых анализов.                            | 8B                 | ~16 GB         | Apache 2.0    | Granite        | [Hugging Face](https://huggingface.co/granite/granite-3.2-8b)                  |
| **granite-3.1-8b**        | Модели с увеличенной производительностью для генерации текста.                | 8B                 | ~16 GB         | Apache 2.0    | Granite        | [Hugging Face](https://huggingface.co/granite/granite-3.1-8b)                  |
| **llama-3.3-70b**        | Модели с 70B параметрами для высококачественной генерации текста.             | 70B                | ~140 GB        | Meta Research | Meta           | [Hugging Face](https://huggingface.co/meta-llama/Llama-3.2-1B)                 |
| **deepseek-r1-0528-qwen3-8b** | 8B              || ~16 GB           ||| [Hugging Face](https://huggingface.co/deepseek/deepseek-r1-0528-qwen3-8b)                                   |
